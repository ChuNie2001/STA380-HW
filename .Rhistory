plot(wss)
#conduct knn with 5 clusters
market_k = kmeans(market_scale, 5, nstart=25)
#plot the cluster
market_p = fviz_cluster(market_k, data = market_scale,
ellipse.type = "euclid", ggtheme = theme_classic(),geom = c("point"))
plot(market_p)
#calculate the mean number of tweet in each cluster
res = aggregate(market_scale, by=list(cluster=market_k$cluster), mean)
res_t <- t(res)
colnames(res_t) <- rownames(res)
rownames(res_t) <- colnames(res)
#we want to remove the correlation row
res_t2 = res_t[-1,]
k = colnames(res_t2)[apply(res_t2,1,which.max)]
clus_features = cbind(rownames(res_t2),k)
# show the final result
clus_features <- data.frame(clus_features)
clus_features %>% arrange(k)
library(tm)
library(tidyverse)
library(slam)
library(proxy)
read<-function(fname){readPlain(elem=list(content=readLines(fname)),id=fname,language='en')}
# load data for all authors
file<-Sys.glob("ReutersC50/C50train/*/*.txt")
author<-lapply(file,read)
name<-file%>%{strsplit(.,'/',fixed=TRUE)}%>%{lapply(.,tail,n=2)}%>%{lapply(.,paste0,collapse='')}%>%unlist
# List authors
a<-"ReutersC50/C50train/"
a_list<-list.dirs(a,full.names=FALSE,recursive=FALSE)
library("corpus")
raw<-Corpus(VectorSource(author))
document<-raw
document<-tm_map(document,content_transformer(tolower))
document<-tm_map(document,content_transformer(removeNumbers))
document<-tm_map(document,content_transformer(removePunctuation))
document<-tm_map(document,content_transformer(stripWhitespace))
stopwords("en")
stopwords("SMART")
?stopwords
# Remove stopwords
document<-tm_map(document,content_transformer(removeWords),stopwords("en"))
# Create a doc-term-matrix
DTM<-DocumentTermMatrix(document)
class(DTM)
DTM<-removeSparseTerms(DTM,0.95)
library(data.table)
# Find frequent terms
col<-colSums(as.matrix(DTM))
length(col)
feature<-data.table(name=attributes(col)$name,count=col)
# List the least 10 frequent words
least_frequent_word10<-feature[order(count)[1:10]]
least_frequent_word10
library(tidyverse)
library(igraph)
library(arules)
library(arulesViz)
#load in the raw data
urlfile9="https://raw.githubusercontent.com/jgscott/STA380/master/data/groceries.txt"
g = read.transactions(file=url(urlfile9),rm.duplicates=TRUE,format="basket",sep=',')
#calculate the support for frequent iteam
g_fq <- eclat (g, parameter = list(supp = 0.07, maxlen = 15))
inspect(g_fq)
#plot the frequency
itemFrequencyPlot(g, topN=5, type="absolute",main="Item Frequency")
grules1 <- apriori(g, parameter = list(support = 0.005, conf = 0.1))
summary(grules1)
grules2 <- apriori(g, parameter = list(support = 0.001, conf = 0.85))
summary(grules2)
#sort the rules based on confidence
grules_s1<-sort(grules2, by="confidence", decreasing=TRUE)
inspect(grules_s1[1:5])
grules_s2<-sort(grules2, by="lift", decreasing=TRUE)
inspect(grules_s2[1:5])
new_rules <- apriori (data=g, parameter=list (supp=0.001,conf = 0.15, minlen=2), appearance = list(default="rhs",lhs="whole milk"), control = list (verbose=F))
new_rules_s <- sort (new_rules, by="confidence", decreasing=TRUE)
inspect(new_rules_s[1:5])
plot(new_rules_s, method="graph", control = list(nodeCol = grey.colors(1), edgeCol = grey(.3),alpha = 1), interactive=FALSE)
library(tidyverse)
library(igraph)
library(arules)
library(arulesViz)
library(tm)
library(tidyverse)
library(slam)
library(proxy)
read<-function(fname){readPlain(elem=list(content=readLines(fname)),id=fname,language='en')}
# load data for all authors
file<-Sys.glob("ReutersC50/C50train/*/*.txt")
author<-lapply(file,read)
name<-file%>%{strsplit(.,'/',fixed=TRUE)}%>%{lapply(.,tail,n=2)}%>%{lapply(.,paste0,collapse='')}%>%unlist
# List authors
a<-"ReutersC50/C50train/"
a_list<-list.dirs(a,full.names=FALSE,recursive=FALSE)
library("corpus")
raw<-Corpus(VectorSource(author))
document<-raw
document<-tm_map(document,content_transformer(tolower))
document<-tm_map(document,content_transformer(removeNumbers))
document<-tm_map(document,content_transformer(removePunctuation))
document<-tm_map(document,content_transformer(stripWhitespace))
stopwords("en")
stopwords("SMART")
?stopwords
# Remove stopwords
document<-tm_map(document,content_transformer(removeWords),stopwords("en"))
# Create a doc-term-matrix
DTM<-DocumentTermMatrix(document)
class(DTM)
DTM<-removeSparseTerms(DTM,0.95)
library(data.table)
# Find frequent terms
col<-colSums(as.matrix(DTM))
length(col)
feature<-data.table(name=attributes(col)$name,count=col)
# List the least 10 frequent words
least_frequent_word10<-feature[order(count)[1:10]]
least_frequent_word10
library(mosaic)
library(quantmod)
library(foreach)
stock2 = c("SCHA", "HDV", "DSI", "SDIV")
price2 = getSymbols(stock, from = "2016-08-13")
for(ticker in stock) {
expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
eval(parse(text=expr))
}
# Combine all the returns in a matrix
return2 = cbind(ClCl(SCHAa),
ClCl(HDVa),
ClCl(DSIa),
ClCl(SDIVa))
library(mosaic)
library(quantmod)
library(foreach)
stock2 = c("SCHA", "HDV", "DSI", "SDIV")
price2 = getSymbols(stock, from = "2016-08-13")
for(ticker in stock) {
expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
eval(parse(text=expr))
}
# Combine all the returns in a matrix
return2 = cbind(ClCl(SCHA),
ClCl(HDVa),
ClCl(DSIa),
ClCl(SDIVa))
SCHa
# Combine all the returns in a matrix
stock2
# Combine all the returns in a matrix
expr
library(mosaic)
library(quantmod)
library(foreach)
stock2 = c("SCHA", "HDV", "DSI", "SDIV")
price2 = getSymbols(stock, from = "2016-08-13")
for(ticker in stock2) {
expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
eval(parse(text=expr))
}
tinytex::install_tinytex()
# P(Y)=0.65
# P(RC)=0.3
# P(TC)=1-P(RC)=0.7
# P(Y/RC)=0.5
Y<-0.65
RC<-0.3
TC<-1-RC
YRC<-0.5
# P(Y)=P(Y/RC)*P(RC)+P(Y/TC)*P(TC)
# P(Y/TC)=(P(Y)-P(Y/RC)*P(RC))/P(TC)
P<-(Y-(YRC*RC))/TC
print(P)
# P(disease)=0.000025
# P(not disease)=1-P(disease)=0.999975
# P(positive/disease)=0.993
# P(negative/not disease)=0.9999
# P(positive/not disease)=1-P(negative/not disease)=0.0001
d<-0.000025
dc<-1-d
pd<-0.993
ndc<-0.9999
pdc<-0.0001
# P(disease/positive)=(P(positive/disease)*P(disease))/((P(positive/disease)*P(disease))+(P(positive/not disease)*P(not disease)))
pdp<-(pd*d)/((pd*d)+(pdc*dc))
print(pdp)
library(readr)
urlfile2<-"https://raw.githubusercontent.com/jgscott/STA380/master/data/billboard.csv"
billboard <- read_csv(url(urlfile2))
head(billboard)
library(dplyr)
library(plyr)
library(ggplot2)
top_lst <- billboard %>%select(c(performer,song,week,week_position,year))%>% filter(year >=1958 & week_position <= 100) %>% group_by(performer,song)%>%ddply(c('performer','song'),summarise,count=length(song))%>% arrange(desc(count))
top_10_lst <-top_lst[1:10,]
top_10_lst
bill1 <- billboard %>% filter(year !=2021 & year !=1958)
unique(bill1[c("performer","song",'year')]) -> unqiue_lst
unqiue_lst %>% group_by(year)%>% tally() -> year_unique_song
ggplot(year_unique_song,aes(x=year,y=n))+geom_line()+ggtitle('Unique Song Per Year')
twh_artist <-top_lst %>% filter(count >= 10)%>%group_by(performer)%>% tally()%>% filter(n>=30)
ggplot(twh_artist, aes(x=performer, y=n,fill = performer)) +geom_bar(stat="identity",show.legend = F)+coord_flip()
library(readr)
urlfile3<-"https://raw.githubusercontent.com/jgscott/STA380/master/data/greenbuildings.csv"
greenbuildings <- read_csv(urlfile3)
head(greenbuildings)
# Plot the distribustion of leasing_rate less than 15%
# From this plot  we know that there are more than 150 properties have 0 % leasing rate, which is
# abnormal, so I consider them as outlier
greenbuildings %>% filter(leasing_rate <=15)%>%ggplot(aes(leasing_rate))+geom_histogram()
greenbuildings %>% filter(leasing_rate >80)%>%ggplot(aes(leasing_rate))+geom_histogram()
greenbuildings %>% filter(green_rating==1) -> GBgreen
greenbuildings %>% filter(green_rating==0) -> GBNotgreen
ggplot()+geom_histogram(data = GBgreen,aes(x=leasing_rate),fill='green')+ggtitle('Green Building Leasing_rate Distribution')
ggplot()+geom_histogram(data=GBNotgreen,aes(x=leasing_rate),fill='orange')+ggtitle('Not Green Building Leasing_rate Distribution')
ggplot()+geom_histogram(data = GBgreen,aes(x=Rent),fill='green')+ggtitle('Green Building Rent Distribution')
ggplot()+geom_histogram(data=GBNotgreen,aes(x=Rent),fill='orange')+ggtitle('Not Green Building Rent Distribution')
ggplot()+geom_boxplot(data = GBgreen,aes(x=Rent),fill='green')+ggtitle('Green Building Rent Distribution')
ggplot()+geom_boxplot(data=GBNotgreen,aes(x=Rent),fill='orange')+ggtitle('Not Green Building Rent Distribution')
median(GBgreen$Rent)
median(GBNotgreen$Rent)
library(reshape2)
rent_green <- greenbuildings%>% select(c(Rent,green_rating))
corr_mat <- round(cor(rent_green),4)
melted_corr_mat <- melt(corr_mat)
ggplot(data = melted_corr_mat, aes(x=Var1, y=Var2,fill=value))+geom_tile()+geom_text(aes(Var2, Var1, label = value),color = "white", size = 4)
green_lm <-lm(Rent~.,data = greenbuildings)
summary(green_lm)
dt <- greenbuildings%>% select(-c(CS_PropertyID,empl_gr,leasing_rate,renovated,LEED,Energystar,green_rating,total_dd_07  ))
corr_mat <- round(cor(dt),2)
melted_corr_mat <- melt(corr_mat)
ggplot(data = melted_corr_mat, aes(x=Var1, y=Var2,fill=value))+geom_tile()+geom_text(aes(Var2, Var1, label = value),color = "white", size = 4)
# An overview of the data set
urlfile4<-"https://raw.githubusercontent.com/jgscott/STA380/master/data/capmetro_UT.csv"
capmetro_UT <- read_csv(url(urlfile4))
head(capmetro_UT)
capmetro_UT%>% group_by(hour_of_day,month,day_of_week)%>%summarise_at(vars(boarding), list(avg_boarding = mean)) -> cap1
ggplot(cap1)+geom_line(aes(x=hour_of_day,y=avg_boarding,color = month))+facet_grid(rows=vars(day_of_week))+scale_x_continuous(breaks=c(6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21))+ggtitle('Average Boarding in Each Hour')
capmetro_UT%>% group_by(hour_of_day,month,day_of_week)%>%summarise_at(vars(alighting), list(avg_alighting = mean)) -> cap2
ggplot(cap2)+geom_line(aes(x=hour_of_day,y=avg_alighting,color = month))+facet_grid(rows=vars(day_of_week))+scale_x_continuous(breaks=c(6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21))+ggtitle('Average Alighting in Each Hour')
capmetro_UT %>% ggplot()+geom_point(aes(x=temperature,y=boarding,color = weekend)) +facet_grid(cols=vars(hour_of_day),scales ="free")+ggtitle('Boarding vs. Temperature')+geom_hline(yintercept=mean(capmetro_UT$temperature), linetype="dashed", color = "black")
capmetro_UT %>% ggplot()+geom_point(aes(x=temperature,y=alighting,color = weekend)) +facet_grid(cols=vars(hour_of_day),scales ="free")+ggtitle('Alighting vs. temperature')+geom_hline(yintercept=mean(capmetro_UT$temperature), linetype="dashed", color = "black")
library("mosaic")
library("quantmod")
library("foreach")
stock=c("SPY","QQQ","VUG","VTI")
getSymbols(stock,from="2016-08-13")
for(ticker in stock) {
expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
eval(parse(text=expr))
}
head(SPYa)
# Combine all the returns in a matrix
return=cbind(ClCl(SPYa),ClCl(QQQa),ClCl(VUGa),ClCl(VTIa))
return<-as.matrix(na.omit(return))
# Compute the returns from the closing prices
pairs(return)
return.day=resample(return,1,orig.ids=FALSE)
wealth<-100000
weight<-c(0.25,0.25,0.25,0.25)
holdings<-weight*wealth
holdings<-holdings*(1+return.day)
set.seed(8)
# Simulate other different possible scenarios
original_wealth=100000
sim1 = foreach(i=1:5000, .combine='rbind') %do% {
wealth = original_wealth
weight = c(0.25, 0.25, 0.25, 0.25)
holdings = weight * wealth
n_days = 20
wealthtracker = rep(0, n_days)
for(today in 1:n_days) {
return.today = resample(return, 1, orig.ids=FALSE)
holdings = holdings + holdings*return.today
wealth = sum(holdings)
wealthtracker[today] = wealth
}
wealthtracker
}
# print the hist
hist(sim1[,n_days],25,main="Portfolio 1")
mean(sim1[,n_days]-original_wealth)
hist(sim1[,n_days]-original_wealth,breaks=30,main="Portfolio 1")
quantile(sim1[,n_days]- original_wealth, prob=0.05)
library(mosaic)
library(quantmod)
library(foreach)
stock2 = c("SCHA", "HDV", "DSI", "SDIV")
price2 = getSymbols(stock2, from = "2016-08-13")
for(ticker in stock2) {
expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
eval(parse(text=expr))
}
# Combine all the returns in a matrix
return2 = cbind(	ClCl(SCHAa),
ClCl(HDVa),
ClCl(DSIa),
ClCl(SDIVa))
return2 = as.matrix(na.omit(return2))
# Compute the returns from the closing prices
pairs(return2)
return.day=resample(return2,1,orig.ids=FALSE)
wealth<-100000
weight<-c(0.25,0.25,0.25,0.25)
holdings<-weight*wealth
holdings<-holdings*(1+return.day)
set.seed(8)
# Simulate other different possible scenarios
original_wealth=100000
sim1 = foreach(i=1:5000, .combine='rbind') %do% {
wealth = original_wealth
weight = c(0.25, 0.25, 0.25, 0.25)
holdings = weight * wealth
n_days = 20
wealthtracker = rep(0, n_days)
for(today in 1:n_days) {
return.today = resample(return2, 1, orig.ids=FALSE)
holdings = holdings + holdings*return.today
wealth = sum(holdings)
wealthtracker[today] = wealth
}
wealthtracker
}
# print the hist
hist(sim1[,n_days],25,main="Portfolio 2")
mean(sim1[,n_days]-original_wealth)
hist(sim1[,n_days]-original_wealth,breaks=30,main="Portfolio 2")
quantile(sim1[,n_days]- original_wealth, prob=0.05)
library(mosaic)
library(quantmod)
library(foreach)
stock3 = c("SPY", "HDV", "DSI", "VTI","QQQ")
price3 = getSymbols(stock, from = "2016-08-13")
for(ticker in stock) {
expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
eval(parse(text=expr))
}
# Combine all the returns in a matrix
return3 = cbind(	ClCl(SPYa),
ClCl(HDVa),
ClCl(DSIa),
ClCl(VTIa),ClCl(QQQa))
return3 = as.matrix(na.omit(return3))
# Compute the returns from the closing prices
pairs(return3)
return.day=resample(return3,1,orig.ids=FALSE)
wealth<-100000
weight<-c(0.2,0.2,0.2,0.2,0.2)
holdings<-weight*wealth
holdings<-holdings*(1+return.day)
set.seed(8)
# Simulate other different possible scenarios
original_wealth=100000
sim1 = foreach(i=1:5000, .combine='rbind') %do% {
wealth = original_wealth
weight = c(0.2, 0.2, 0.2, 0.2,0.2)
holdings = weight * wealth
n_days = 20
wealthtracker = rep(0, n_days)
for(today in 1:n_days) {
return.today = resample(return3, 1, orig.ids=FALSE)
holdings = holdings + holdings*return.today
wealth = sum(holdings)
wealthtracker[today] = wealth
}
wealthtracker
}
# print the hist
hist(sim1[,n_days],25,main="Portfolio 3")
mean(sim1[,n_days]-original_wealth)
hist(sim1[,n_days]-original_wealth,breaks=30,main="Portfolio 3")
quantile(sim1[,n_days]- original_wealth, prob=0.05)
library(readr)
urlfile6<-"https://raw.githubusercontent.com/jgscott/STA380/master/data/wine.csv"
wine <- read_csv(url(urlfile6))
head(wine)
library(tidyverse)
library(ggplot2)
library(factoextra)
library(dplyr)
library(cluster)
library(GGally)
library(caret)
#K-mean - color
wine_scale <- wine%>% select(-c(quality,color)) %>% scale()
pam1 <-pam(wine_scale,k=2)
pam1$clusinfo[,1] # Get how many data in each cluster
#wine%>%group_by(color)%>% tally()%>% mutate(cluster = pam1$clusinfo[,1]) # Compare the real category and cluster results
wine1 <-wine%>% mutate(clr = ifelse(color == 'red',1,2))
t1 <- xtabs(~pam1$clustering + wine1$clr)
confusionMatrix(t1)
pamclust <-wine %>% mutate(cluster = as.factor(pam1$clustering))
fviz_cluster(pam1,data = wine_scale)
pamclust %>% ggpairs(col = c("total.sulfur.dioxide","density","pH","volatile.acidity"), aes(color = cluster,alpha=0.6))
pamclust%>%ggplot(aes(total.sulfur.dioxide,volatile.acidity, color=color, shape=cluster))+geom_point(size=4,alpha=0.6)
# K-mean - quality
pam2 <-pam(wine_scale,k=7)
pam2$clusinfo[,1] # Get how many data in each cluster
#wine%>%group_by(quality)%>% tally()%>% mutate(cluster2 = pam2$clusinfo[,1]) # Compare the real category and cluster results
wine1 <-wine%>% mutate(quali = quality-2)
tab <-xtabs(~pam2$clustering + wine1$quali)
confusionMatrix(tab)
pamclust2 <-wine %>% mutate(cluster = as.factor(pam2$clustering))
fviz_cluster(pam2,data = wine_scale)
pamclust2 %>% ggpairs(col = c("total.sulfur.dioxide","density","pH","volatile.acidity"), aes(color = cluster,alpha=0.6))
pamclust2 %>%ggplot(aes(total.sulfur.dioxide,volatile.acidity, color=as.factor(quality), shape=cluster))+geom_point(size=4,alpha=0.5)+scale_shape_manual(values=c(15,16,17,18,19,20,8))
wine_data <- wine %>% select(-c(color,quality))
wine_pca <- prcomp(wine_data,center = T,scale=T)
# PCA - Color
## color in original classification in rotated axis
#wine %>% mutate(PC1=wine_pca$x[, 1], PC2=wine_pca$x[, 2]) %>%ggplot(aes(PC1, PC2, color=color)) + geom_point() + coord_fixed()
## color in original classification shape in PCA cluster
pcaClust <- kmeans(wine_pca$x[,1:9],2, nstart=25) # Keep first 9 pc to gain more than 95% variance
wine1 <-wine%>% mutate(clr = ifelse(color == 'red',1,2))
t2 <- xtabs(~pcaClust$cluster + wine1$clr)
confusionMatrix(t2)
qplot(wine_pca$x[,1], wine_pca$x[,2], color=factor(wine$color),alpha=0.5, shape=factor(pcaClust$cluster), xlab='PC1', ylab='PC2')
# PCA - quality
## color in original classification in rotated axis
#wine %>% mutate(PC1=wine_pca$x[, 1], PC2=wine_pca$x[, 2]) %>%ggplot(aes(PC1, PC2, color=as.factor(quality))) + geom_point() + coord_fixed()
## color in original classification shape in PCA cluster
pcaClust1 <- kmeans(wine_pca$x[,1:9],7, nstart=25)# Keep first 9 pc to gain more than 95% variance
wine1 <-wine%>% mutate(quali = quality-2)
tab2 <- xtabs(~pcaClust1$cluster + wine1$quali)
confusionMatrix(tab2)
qplot(wine_pca$x[,1], wine_pca$x[,2], color=as.factor(wine$quality),alpha=0.5, shape=factor(pcaClust1$cluster), xlab='PC1', ylab='PC2')+scale_shape_manual(values=c(15,16,17,18,19,20,8))
urlfile7="https://raw.githubusercontent.com/jgscott/STA380/master/data/social_marketing.csv"
social_marketing<-read_csv(url(urlfile7))
library(tidyverse)
library(cluster)
library(factoextra)
library(NbClust)
library(corrplot)
library(dplyr)
market <- social_marketing
#take a look at the data structure itself
str(market)
#select everything except ID and check the correlation among them
correlation <- cor(market[c(2:37)])
corrplot(correlation, method = 'color', tl.cex = 0.7)
#scale the data and exclude chatter, spam and adult
market_scale <- scale(market[,3:35], center=TRUE, scale=TRUE)
center_m = attr(market_scale,"scaled:center")
scale_m = attr(market_scale,"scaled:scale")
wss = fviz_nbclust(market_scale, kmeans, method = "wss")
plot(wss)
#conduct knn with 5 clusters
market_k = kmeans(market_scale, 5, nstart=25)
#plot the cluster
market_p = fviz_cluster(market_k, data = market_scale,
ellipse.type = "euclid", ggtheme = theme_classic(),geom = c("point"))
plot(market_p)
#calculate the mean number of tweet in each cluster
res = aggregate(market_scale, by=list(cluster=market_k$cluster), mean)
res_t <- t(res)
colnames(res_t) <- rownames(res)
rownames(res_t) <- colnames(res)
#we want to remove the correlation row
res_t2 = res_t[-1,]
k = colnames(res_t2)[apply(res_t2,1,which.max)]
clus_features = cbind(rownames(res_t2),k)
# show the final result
clus_features <- data.frame(clus_features)
clus_features %>% arrange(k)
library(tm)
library(tidyverse)
library(slam)
library(proxy)
read<-function(fname){readPlain(elem=list(content=readLines(fname)),id=fname,language='en')}
# load data for all authors
file<-Sys.glob("ReutersC50/C50train/*/*.txt")
author<-lapply(file,read)
name<-file%>%{strsplit(.,'/',fixed=TRUE)}%>%{lapply(.,tail,n=2)}%>%{lapply(.,paste0,collapse='')}%>%unlist
# List authors
a<-"ReutersC50/C50train/"
a_list<-list.dirs(a,full.names=FALSE,recursive=FALSE)
library("corpus")
raw<-Corpus(VectorSource(author))
document<-raw
document<-tm_map(document,content_transformer(tolower))
document<-tm_map(document,content_transformer(removeNumbers))
document<-tm_map(document,content_transformer(removePunctuation))
document<-tm_map(document,content_transformer(stripWhitespace))
stopwords("en")
stopwords("SMART")
?stopwords
# Remove stopwords
document<-tm_map(document,content_transformer(removeWords),stopwords("en"))
# Create a doc-term-matrix
DTM<-DocumentTermMatrix(document)
class(DTM)
DTM<-removeSparseTerms(DTM,0.95)
library(data.table)
# Find frequent terms
col<-colSums(as.matrix(DTM))
length(col)
feature<-data.table(name=attributes(col)$name,count=col)
# List the least 10 frequent words
least_frequent_word10<-feature[order(count)[1:10]]
least_frequent_word10
library(tidyverse)
library(igraph)
library(arules)
library(arulesViz)
#load in the raw data
urlfile9="https://raw.githubusercontent.com/jgscott/STA380/master/data/groceries.txt"
g = read.transactions(file=url(urlfile9),rm.duplicates=TRUE,format="basket",sep=',')
#calculate the support for frequent iteam
g_fq <- eclat (g, parameter = list(supp = 0.07, maxlen = 15))
inspect(g_fq)
